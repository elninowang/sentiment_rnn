{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN 使用RNN来进行情绪分析\n",
    "\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis. Using an RNN rather than a feedfoward network is more accurate since we can include information about the *sequence* of words. Here we'll use a dataset of movie reviews, accompanied by labels.\n",
    "\n",
    "在这本笔记本中，您将实施执行情绪分析的循环神经网络。 使用RNN而不是feedfoward网络更准确，因为我们可以包括有关*sequence*的信息。 在这里，我们将使用电影评论的数据集，并附有标签。\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "该网络的架构如下所示。\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Here, we'll pass in words to an embedding layer. We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the word2vec lesson. You can actually train up an embedding with word2vec and use it here. But it's good enough to just have an embedding layer and let the network learn the embedding table on it's own.\n",
    "\n",
    "在这里，我们将传入一个嵌入层。 我们需要一个嵌入层，因为我们有数万个单词，所以我们需要比单热编码向量更有效地表示输入数据。 你应该从word2vec课程中看到过。 实际上你可以用word2vec来训练一个嵌入，并在这里使用它。 但是，只要拥有一个嵌入层，让网络学习嵌入表就可以了。\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. We're using the sigmoid because we're trying to predict if this text has positive or negative sentiment. The output layer will just be a single unit then, with a sigmoid activation function.\n",
    "\n",
    "从嵌入层，新的表示将被传递给LSTM单元。 这些将添加到网络的重复连接，因此我们可以包括关于数据中单词序列的信息。 最后，LSTM单元将在这里进入S形输出层。 我们正在使用sigmoid，因为我们试图预测这个文本是否有积极或消极的情绪。 输出层只是一个单一的单元，然后具有S形激活功能。\n",
    "\n",
    "We don't care about the sigmoid outputs except for the very last one, we can ignore the rest. We'll calculate the cost from the output of the last step and the training label.\n",
    "\n",
    "我们不关心Sigmoid输出，除了最后一个，我们可以忽略其余的。 我们将从最后一步的输出和培训标签计算成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "labels = labels.split('\\n')\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing 数据预处理\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "构建神经网络模型的第一步是将您的数据转化为正确的形式进入网络。 由于我们使用嵌入层，我们需要用一个整数对每个单词进行编码。 我们也想清理一下。\n",
    "\n",
    "You can see an example of the reviews data above. We'll want to get rid of those periods. Also, you might notice that the reviews are delimited with newlines `\\n`. To deal with those, I'm going to split the text into each review using `\\n` as the delimiter. Then I can combined all the reviews back together into one big string.\n",
    "\n",
    "您可以看到上面的评论数据的例子。 我们想要摆脱那些时期。 另外，您可能会注意到，这些评论用换行符`\\ n`分隔。 为了处理这些，我将使用`\\ n`作为分隔符将文本分割成每个评论。 然后我可以将所有的评论结合在一起，形成一个大字符串。\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words.\n",
    "\n",
    "首先，我们删除所有的标点符号。 然后获取所有没有换行符的文本，并将其分成单个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t    story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly    homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the st'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words 对多个单词进行编码\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "嵌入式查找要求我们将整数传递给我们的网络。 最简单的方法是创建将词表中的单词映射为整数的字典。 然后我们可以将每个评论转换成整数，以便将它们传递到网络中。\n",
    "\n",
    "> **Exercise:** Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.\n",
    "> Also, convert the reviews to integers and store the reviews in a new list called `reviews_ints`. \n",
    "\n",
    "> **练习：** 现在你要用整数来编码这些单词。 构建一个将单词映射到整数的字典。 稍后我们将使用零填充输入向量，所以确保整数 **start at 1, not 0**。\n",
    ">此外，将评论转换为整数，并将评论存储在名为`reviews_ints`的新列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Create your dictionary that maps vocab words to integers here\n",
    "vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels 对多个标签进行编码\n",
    "\n",
    "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n",
    "\n",
    "我们的标签是“positive”或“negative”。 要在我们的网络中使用这些标签，我们需要将它们转换为0和1。\n",
    "\n",
    "> **Exercise:** Convert labels from `positive` and `negative` to 1 and 0, respectively.\n",
    "\n",
    "> **练习：** 将标签从`positive`和`negative`分别转换为1和0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = np.array([1 if each=='positive' else 0 for each in labels])\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you built `labels` correctly, you should see the next output.\n",
    "\n",
    "如果你正确地构建了`labels`，你应该看到下一个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a couple issues here. We seem to have one review with zero length. And, the maximum review length is way too many steps for our RNN. Let's truncate to 200 steps. For reviews shorter than 200, we'll pad with 0s. For reviews longer than 200, we can truncate them to the first 200 characters.\n",
    "\n",
    "好的，这里有几个问题。 我们似乎有一个零长度的审查。 而且，我们的RNN的最大审查时间是太多的步骤。 让我们截断到200步。 对于小于200的评论，我们将用0填写。 对于超过200次的评论，我们可以将其截断为前200个字符。\n",
    "\n",
    "> **Exercise:** First, remove the review with zero length from the `reviews_ints` list.\n",
    "\n",
    "> **练习：**首先，从`reviews_ints`列表中删除零长度的评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Filter out that review with 0 length\n",
    "print(len(reviews_ints))\n",
    "reviews_ints = [each for each in reviews_ints if len(each) > 0]\n",
    "print(len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Now, create an array `features` that contains the data we'll pass to the network. The data should come from `review_ints`, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. For reviews longer than 200, use on the first 200 words as the feature vector.\n",
    "\n",
    "> **练习：**现在，创建一个包含我们传递给网络的数据的数组`features`。 数据应该来自`review_ints`，因为我们想把整数提供给网络。 每行应该是200元素长。 对于短于200个字的评论，左键为0。 也就是说，如果审查是`['best', 'movie', 'ever']`，`[117, 18, 128]` 作为整数，行将看起来像 `[0, 0, 0, ..., 0, 117, 18, 128]` 对于超过200次的评论，使用前200个单词作为特征向量。\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data.\n",
    "\n",
    "这不是微不足道的，有一些方法来做到这一点。 但是，如果您要建立自己的深入学习网络，那么您将不得不习惯于准备数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you build features correctly, it should look like that cell output below.\n",
    "\n",
    "如果正确构建功能，它应该看起来像下面的单元格输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 21025,   308,     6,\n",
       "            3,  1050,   207,     8,  2138,    32,     1,   171,    57,\n",
       "           15,    49,    81,  5785,    44,   382,   110,   140,    15,\n",
       "         5194,    60,   154,     9,     1,  4975,  5852,   475,    71,\n",
       "            5,   260,    12, 21025,   308,    13,  1978,     6,    74,\n",
       "         2395],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    63,     4,     3,   125,\n",
       "           36,    47,  7472,  1395,    16,     3,  4181,   505,    45,\n",
       "           17],\n",
       "       [22382,    42, 46418,    15,   706, 17139,  3389,    47,    77,\n",
       "           35,  1819,    16,   154,    19,   114,     3,  1305,     5,\n",
       "          336,   147,    22,     1,   857,    12,    70,   281,  1168,\n",
       "          399,    36,   120,   283,    38,   169,     5,   382,   158,\n",
       "           42,  2269,    16,     1,   541,    90,    78,   102,     4,\n",
       "            1,  3244,    15,    43,     3,   407,  1068,   136,  8055,\n",
       "           44,   182,   140,    15,  3043,     1,   320,    22,  4818,\n",
       "        26224,   346,     5,  3090,  2092,     1, 18839, 17939,    42,\n",
       "         8055,    46,    33,   236,    29,   370,     5,   130,    56,\n",
       "           22,     1,  1928,     7,     7,    19,    48,    46,    21,\n",
       "           70,   344,     3,  2099,     5,   408,    22,     1,  1928,\n",
       "           16],\n",
       "       [ 4505,   505,    15,     3,  3342,   162,  8312,  1652,     6,\n",
       "         4819,    56,    17,  4504,  5616,   140, 11725,     5,   996,\n",
       "         4919,  2933,  4462,   566,  1201,    36,     6,  1518,    96,\n",
       "            3,   744,     4, 26225,    13,     5,    27,  3461,     9,\n",
       "        10625,     4,     8,   111,  3013,     5,     1,  1027,    15,\n",
       "            3,  4390,    82,    22,  2049,     6,  4462,   538,  2764,\n",
       "         7073, 37443,    41,   463,     1,  8312, 46419,   302,   123,\n",
       "           15,  4221,    19,  1667,   922,     1,  1652,     6,  6129,\n",
       "        19871,    34,     1,   980,  1751, 22383,   646, 24104,    27,\n",
       "          106, 11726,    13, 14045, 15097, 17940,  2457,   466, 21027,\n",
       "           36,  3266,     1,  6365,  1020,    45,    17,  2695,  2499,\n",
       "           33],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,   520,   119,   113,    34,\n",
       "        16372,  1816,  3737,   117,   885, 21030,   721,    10,    28,\n",
       "          124,   108,     2,   115,   137,     9,  1623,  7691,    26,\n",
       "          330,     5,   589,     1,  6130,    22,   386,     6,     3,\n",
       "          349,    15,    50,    15,   231,     9,  7473, 11399,     1,\n",
       "          191,    22,  8966,     6,    82,   880,   101,   111,  3584,\n",
       "            4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:5,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test 训练，验证，测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "我们的数据很好，我们将其分为培训，验证和测试集。\n",
    "\n",
    "> **Exercise:** Create the training, validation, and test sets here. You'll need to create sets for the features and the labels, `train_x` and `train_y` for example. Define a split fraction, `split_frac` as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. The rest of the data will be split in half to create the validation and testing data.\n",
    "\n",
    "> **练习：**在这里创建训练，验证和测试集。 您将需要为功能和标签创建集合，例如`train_x`和`train_y`。 定义一个分数分数，`split_frac`作为保留在训练集中的数据的一部分。 通常设置为0.8或0.9。 剩下的数据将被分成两半，以创建验证和测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With train, validation, and text fractions of 0.8, 0.1, 0.1, the final shapes should look like:\n",
    "\n",
    "具有0.8,0.1,0.1的训练，验证和文本分数，最终形状应如下所示：\n",
    "```\n",
    "                    Feature Shapes:\n",
    "Train set: \t\t (20000, 200) \n",
    "Validation set: \t(2500, 200) \n",
    "Test set: \t\t  (2500, 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph 建立图表\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate\n",
    "\n",
    "在这里，我们将构建图。 首先，定义超参数。\n",
    "\n",
    "* `lstm_size`：LSTM单元格中隐藏图层中的单位数。 通常更大的是更好的性能明智。 常用值为128,256,512等\n",
    "* `lstm_layers`：网络中的LSTM层数。 我从1开始，然后如果我不适合，添加更多。\n",
    "* `batch_size`：在一个培训通行证中提供网络的评论数量。 通常这应该设置为尽可能高，没有内存不足。\n",
    "* `learning_rate`：学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 3\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability.\n",
    "\n",
    "对于网络本身，我们将传递我们的200个元素长的评估向量。 每批将是 `batch_size` 向量。 我们还将在LSTM层使用辍学，所以我们将为保留概率创建一个占位符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Create the `inputs_`, `labels_`, and drop out `keep_prob` placeholders using `tf.placeholder`. `labels_` needs to be two-dimensional to work with some functions later.  Since `keep_prob` is a scalar (a 0-dimensional tensor), you shouldn't provide a size to `tf.placeholder`.\n",
    "\n",
    "> **练习：** 创建`inputs_`，`labels_`，并使用`tf.placeholder`退出`keep_prob`占位符。 `labels_`需要二维以后才能使用某些功能。 因为`keep_prob`是一个标量（一个0维张量），所以你不应该为`tf.placeholder`提供一个大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None,None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None,None], name='inputs')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 嵌入\n",
    "\n",
    "Now we'll add an embedding layer. We need to do this because there are 74000 words in our vocabulary. It is massively inefficient to one-hot encode our classes here. You should remember dealing with this problem from the word2vec lesson. Instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using word2vec, then load it here. But, it's fine to just make a new layer and let the network learn the weights.\n",
    "\n",
    "现在我们将添加一个嵌入层。 我们需要这样做，因为我们的词汇中有74000个单词。 在这里对我们的课程进行一次热编码是非常有效的。 你应该记住从word2vec课程处理这个问题。 而不是单热编码，我们可以拥有嵌入层，并将该层用作查找表。 您可以使用word2vec训练一个嵌入层，然后将其加载到此处。 但是，只需创建一个新层，让网络学习权重就可以了。\n",
    "\n",
    "> **Exercise:** Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). This function takes the embedding matrix and an input tensor, such as the review vectors. Then, it'll return another tensor with the embedded vectors. So, if the embedding layer has 200 units, the function will return a tensor with size [batch_size, 200].\n",
    "\n",
    "> **练习：** 将嵌入式查找矩阵创建为`tf.Variable`。 使用该嵌入矩阵获取嵌入的向量以 [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). 传递给LSTM单元格。 该函数采用嵌入矩阵和输入张量，如评估向量。 然后，它将返回另一个与嵌入向量的张量。 因此，如果嵌入层有200个单位，则函数将返回尺寸为[batch_size，200]的张量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell   LSTM细胞\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Next, we'll create our LSTM cells to use in the recurrent network ([TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)). Here we are just defining what the cells look like. This isn't actually building the graph, just defining the type of cells we want in our graph.\n",
    "\n",
    "接下来，我们将创建我们的LSTM单元格，用于经常性网络（[TensorFlow文档](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)）。 这里我们只是定义单元格的外观。 这不是实际构建图，只是在图中定义我们想要的单元格类型。\n",
    "\n",
    "To create a basic LSTM cell for the graph, you'll want to use `tf.contrib.rnn.BasicLSTMCell`. Looking at the function documentation:\n",
    "\n",
    "要为图形创建一个基本的LSTM单元格，您需要使用`tf.contrib.rnn.BasicLSTMCell`。 查看功能文档：\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)\n",
    "```\n",
    "\n",
    "you can see it takes a parameter called `num_units`, the number of units in the cell, called `lstm_size` in this code. So then, you can write something like \n",
    "\n",
    "您可以看到它在该代码中使用了一个名为`num_units`的参数，单元格中的单位数，称为`lstm_size`。 那么，你可以写一些类似的东西\n",
    "\n",
    "```\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "to create an LSTM cell with `num_units`. Next, you can add dropout to the cell with `tf.contrib.rnn.DropoutWrapper`. This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make your network better with almost no effort! So you'd do something like\n",
    "\n",
    "用`num_units` 创建一个LSTM单元格。 接下来，您可以使用`tf.contrib.rnn.DropoutWrapper`向单元格添加退出。 这只是将单元格包装在另一个单元格中，但是将输出和/或输出添加到输出中。 这是一个非常方便的方式，使您的网络更好，几乎没有任何努力！ 所以你会做一些事情\n",
    "\n",
    "```\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "```\n",
    "\n",
    "Most of the time, your network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with `tf.contrib.rnn.MultiRNNCell`:\n",
    "\n",
    "大多数时候，您的网络将具有更好的性能，更多的层。 这就是深入学习的神奇之处，增加更多层次让网络学习真正复杂的关系。 再次，有一个简单的方式来创建具有`tf.contrib.rnn.MultiRNNCell`的多层LSTM单元格：\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "```\n",
    "\n",
    "Here, `[drop] * lstm_layers` creates a list of cells (`drop`) that is `lstm_layers` long. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "这里，`[drop] * lstm_layers`创建一个长度为lstm_layers的单元格列表（`drop`）。 `MultiRNNCell`包装器将其构建到多个RNN单元格中，一个用于列表中的每个单元格。\n",
    "\n",
    "So the final cell you're using in the network is actually multiple (or just one) LSTM cells with dropout. But it all works the same from an achitectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    "因此，您在网络中使用的最后一个单元格实际上是多个（或只有一个）具有删除的LSTM单元格。 但是，从建筑的角度来看，它们都是一样的，只是一个更复杂的单元格图形。\n",
    "\n",
    "> **Exercise:** Below, use `tf.contrib.rnn.BasicLSTMCell` to create an LSTM cell. Then, add drop out to it with `tf.contrib.rnn.DropoutWrapper`. Finally, create multiple LSTM layers with `tf.contrib.rnn.MultiRNNCell`.\n",
    "\n",
    "> **练习：** 下面，使用`tf.contrib.rnn.BasicLSTMCell`创建一个LSTM单元格。 然后，使用`tf.contrib.rnn.DropoutWrapper`添加到它。 最后，使用`tf.contrib.rnn.MultiRNNCell`创建多个LSTM图层。\n",
    "\n",
    "Here is [a tutorial on building RNNs](https://www.tensorflow.org/tutorials/recurrent) that will help you out.\n",
    "\n",
    "这是 [一个关于建立RNN的教程](https://www.tensorflow.org/tutorials/recurrent)，这将有助于您。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm =  tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN forward pass RNN前向传递\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Now we need to actually run the data through the RNN nodes. You can use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to do this. You'd pass in the RNN cell you created (our multiple layered LSTM `cell` for instance), and the inputs to the network.\n",
    "\n",
    "现在我们需要通过RNN节点实际运行数据。 您可以使用[`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)来执行此操作。 你会传入你创建的RNN单元格（例如我们的多层LSTM `cell`），以及对网络的输入。\n",
    "\n",
    "```\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "```\n",
    "\n",
    "Above I created an initial state, `initial_state`, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. `tf.nn.dynamic_rnn` takes care of most of the work for us. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer.\n",
    "\n",
    "上面我创建了一个初始状态`initial_state`，传递给RNN。 这是在连续时间步长中在隐藏层之间传递的单元格状态。 `tf.nn.dynamic_rnn`为我们照顾大部分的工作。 我们通过我们的单元格和输入到单元格，然后它展开和其他一切为我们。 它返回每个时间步的输出和隐藏层的final_state。\n",
    "\n",
    "> **Exercise:** Use `tf.nn.dynamic_rnn` to add the forward pass through the RNN. Remember that we're actually passing in vectors from the embedding layer, `embed`.\n",
    "\n",
    "> **练习：**使用`tf.nn.dynamic_rnn`添加通过RNN的前进路径。 记住，我们实际上是从嵌入层 `embed` 传递向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 输出\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with `outputs[:, -1]`, the calculate the cost from that and `labels_`.\n",
    "\n",
    "我们只关心最后的输出，我们将使用它作为我们的情绪预测。 所以我们需要用`outputs[:, -1]`来获取最后一个输出，计算出它的代价和 `labels_`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy 验证准确率\n",
    "\n",
    "Here we can add a few nodes to calculate the accuracy which we'll use in the validation pass.\n",
    "\n",
    "这里我们可以添加几个节点来计算我们将在验证过程中使用的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching 分批\n",
    "\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`.\n",
    "\n",
    "这是从我们的数据返回批次的一个简单的功能。 首先它删除数据，使我们只有完整的批次。 然后它遍历 `x` 和 `y` 数组，并从大小为`[batch_size]`的数组返回分片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 训练\n",
    "\n",
    "Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. Before you run this, make sure the `checkpoints` directory exists.\n",
    "\n",
    "以下是典型的培训代码。 如果你想自己做这个，可以自由删除所有这些代码，并自己实现。 在运行此操作之前，请确保 `checkpoints` 目录存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15 Iteration: 5 Train loss: 0.233\n",
      "Epoch: 0/15 Iteration: 10 Train loss: 0.239\n",
      "Epoch: 0/15 Iteration: 15 Train loss: 0.229\n",
      "Epoch: 0/15 Iteration: 20 Train loss: 0.196\n",
      "Epoch: 0/15 Iteration: 25 Train loss: 0.238\n",
      "Val acc: 0.595\n",
      "Epoch: 0/15 Iteration: 30 Train loss: 0.213\n",
      "Epoch: 0/15 Iteration: 35 Train loss: 0.190\n",
      "Epoch: 0/15 Iteration: 40 Train loss: 0.194\n",
      "Epoch: 1/15 Iteration: 45 Train loss: 0.174\n",
      "Epoch: 1/15 Iteration: 50 Train loss: 0.208\n",
      "Val acc: 0.650\n",
      "Epoch: 1/15 Iteration: 55 Train loss: 0.183\n",
      "Epoch: 1/15 Iteration: 60 Train loss: 0.146\n",
      "Epoch: 1/15 Iteration: 65 Train loss: 0.143\n",
      "Epoch: 1/15 Iteration: 70 Train loss: 0.179\n",
      "Epoch: 1/15 Iteration: 75 Train loss: 0.164\n",
      "Val acc: 0.703\n",
      "Epoch: 1/15 Iteration: 80 Train loss: 0.349\n",
      "Epoch: 2/15 Iteration: 85 Train loss: 0.292\n",
      "Epoch: 2/15 Iteration: 90 Train loss: 0.266\n",
      "Epoch: 2/15 Iteration: 95 Train loss: 0.248\n",
      "Epoch: 2/15 Iteration: 100 Train loss: 0.250\n",
      "Val acc: 0.578\n",
      "Epoch: 2/15 Iteration: 105 Train loss: 0.245\n",
      "Epoch: 2/15 Iteration: 110 Train loss: 0.238\n",
      "Epoch: 2/15 Iteration: 115 Train loss: 0.249\n",
      "Epoch: 2/15 Iteration: 120 Train loss: 0.236\n",
      "Epoch: 3/15 Iteration: 125 Train loss: 0.239\n",
      "Val acc: 0.553\n",
      "Epoch: 3/15 Iteration: 130 Train loss: 0.236\n",
      "Epoch: 3/15 Iteration: 135 Train loss: 0.182\n",
      "Epoch: 3/15 Iteration: 140 Train loss: 0.314\n",
      "Epoch: 3/15 Iteration: 145 Train loss: 0.252\n",
      "Epoch: 3/15 Iteration: 150 Train loss: 0.260\n",
      "Val acc: 0.500\n",
      "Epoch: 3/15 Iteration: 155 Train loss: 0.252\n",
      "Epoch: 3/15 Iteration: 160 Train loss: 0.251\n",
      "Epoch: 4/15 Iteration: 165 Train loss: 0.239\n",
      "Epoch: 4/15 Iteration: 170 Train loss: 0.247\n",
      "Epoch: 4/15 Iteration: 175 Train loss: 0.239\n",
      "Val acc: 0.590\n",
      "Epoch: 4/15 Iteration: 180 Train loss: 0.235\n",
      "Epoch: 4/15 Iteration: 185 Train loss: 0.203\n",
      "Epoch: 4/15 Iteration: 190 Train loss: 0.173\n",
      "Epoch: 4/15 Iteration: 195 Train loss: 0.177\n",
      "Epoch: 4/15 Iteration: 200 Train loss: 0.170\n",
      "Val acc: 0.733\n",
      "Epoch: 5/15 Iteration: 205 Train loss: 0.130\n",
      "Epoch: 5/15 Iteration: 210 Train loss: 0.160\n",
      "Epoch: 5/15 Iteration: 215 Train loss: 0.149\n",
      "Epoch: 5/15 Iteration: 220 Train loss: 0.125\n",
      "Epoch: 5/15 Iteration: 225 Train loss: 0.159\n",
      "Val acc: 0.714\n",
      "Epoch: 5/15 Iteration: 230 Train loss: 0.138\n",
      "Epoch: 5/15 Iteration: 235 Train loss: 0.111\n",
      "Epoch: 5/15 Iteration: 240 Train loss: 0.154\n",
      "Epoch: 6/15 Iteration: 245 Train loss: 0.109\n",
      "Epoch: 6/15 Iteration: 250 Train loss: 0.116\n",
      "Val acc: 0.798\n",
      "Epoch: 6/15 Iteration: 255 Train loss: 0.093\n",
      "Epoch: 6/15 Iteration: 260 Train loss: 0.099\n",
      "Epoch: 6/15 Iteration: 265 Train loss: 0.090\n",
      "Epoch: 6/15 Iteration: 270 Train loss: 0.082\n",
      "Epoch: 6/15 Iteration: 275 Train loss: 0.117\n",
      "Val acc: 0.788\n",
      "Epoch: 6/15 Iteration: 280 Train loss: 0.112\n",
      "Epoch: 7/15 Iteration: 285 Train loss: 0.073\n",
      "Epoch: 7/15 Iteration: 290 Train loss: 0.097\n",
      "Epoch: 7/15 Iteration: 295 Train loss: 0.069\n",
      "Epoch: 7/15 Iteration: 300 Train loss: 0.079\n",
      "Val acc: 0.831\n",
      "Epoch: 7/15 Iteration: 305 Train loss: 0.077\n",
      "Epoch: 7/15 Iteration: 310 Train loss: 0.067\n",
      "Epoch: 7/15 Iteration: 315 Train loss: 0.073\n",
      "Epoch: 7/15 Iteration: 320 Train loss: 0.109\n",
      "Epoch: 8/15 Iteration: 325 Train loss: 0.091\n",
      "Val acc: 0.770\n",
      "Epoch: 8/15 Iteration: 330 Train loss: 0.096\n",
      "Epoch: 8/15 Iteration: 335 Train loss: 0.093\n",
      "Epoch: 8/15 Iteration: 340 Train loss: 0.102\n",
      "Epoch: 8/15 Iteration: 345 Train loss: 0.110\n",
      "Epoch: 8/15 Iteration: 350 Train loss: 0.099\n",
      "Val acc: 0.822\n",
      "Epoch: 8/15 Iteration: 355 Train loss: 0.069\n",
      "Epoch: 8/15 Iteration: 360 Train loss: 0.098\n",
      "Epoch: 9/15 Iteration: 365 Train loss: 0.037\n",
      "Epoch: 9/15 Iteration: 370 Train loss: 0.089\n",
      "Epoch: 9/15 Iteration: 375 Train loss: 0.084\n",
      "Val acc: 0.802\n",
      "Epoch: 9/15 Iteration: 380 Train loss: 0.064\n",
      "Epoch: 9/15 Iteration: 385 Train loss: 0.062\n",
      "Epoch: 9/15 Iteration: 390 Train loss: 0.059\n",
      "Epoch: 9/15 Iteration: 395 Train loss: 0.062\n",
      "Epoch: 9/15 Iteration: 400 Train loss: 0.056\n",
      "Val acc: 0.765\n",
      "Epoch: 10/15 Iteration: 405 Train loss: 0.067\n",
      "Epoch: 10/15 Iteration: 410 Train loss: 0.070\n",
      "Epoch: 10/15 Iteration: 415 Train loss: 0.066\n",
      "Epoch: 10/15 Iteration: 420 Train loss: 0.130\n",
      "Epoch: 10/15 Iteration: 425 Train loss: 0.245\n",
      "Val acc: 0.667\n",
      "Epoch: 10/15 Iteration: 430 Train loss: 0.222\n",
      "Epoch: 10/15 Iteration: 435 Train loss: 0.208\n",
      "Epoch: 10/15 Iteration: 440 Train loss: 0.356\n",
      "Epoch: 11/15 Iteration: 445 Train loss: 0.268\n",
      "Epoch: 11/15 Iteration: 450 Train loss: 0.272\n",
      "Val acc: 0.656\n",
      "Epoch: 11/15 Iteration: 455 Train loss: 0.261\n",
      "Epoch: 11/15 Iteration: 460 Train loss: 0.234\n",
      "Epoch: 11/15 Iteration: 465 Train loss: 0.256\n",
      "Epoch: 11/15 Iteration: 470 Train loss: 0.166\n",
      "Epoch: 11/15 Iteration: 475 Train loss: 0.198\n",
      "Val acc: 0.788\n",
      "Epoch: 11/15 Iteration: 480 Train loss: 0.227\n",
      "Epoch: 12/15 Iteration: 485 Train loss: 0.073\n",
      "Epoch: 12/15 Iteration: 490 Train loss: 0.121\n",
      "Epoch: 12/15 Iteration: 495 Train loss: 0.073\n",
      "Epoch: 12/15 Iteration: 500 Train loss: 0.040\n",
      "Val acc: 0.842\n",
      "Epoch: 12/15 Iteration: 505 Train loss: 0.018\n",
      "Epoch: 12/15 Iteration: 510 Train loss: 0.003\n",
      "Epoch: 12/15 Iteration: 515 Train loss: 0.001\n",
      "Epoch: 12/15 Iteration: 520 Train loss: 0.001\n",
      "Epoch: 13/15 Iteration: 525 Train loss: 0.086\n",
      "Val acc: 0.884\n",
      "Epoch: 13/15 Iteration: 530 Train loss: 0.036\n",
      "Epoch: 13/15 Iteration: 535 Train loss: 0.021\n",
      "Epoch: 13/15 Iteration: 540 Train loss: 0.031\n",
      "Epoch: 13/15 Iteration: 545 Train loss: 0.018\n",
      "Epoch: 13/15 Iteration: 550 Train loss: 0.005\n",
      "Val acc: 0.608\n",
      "Epoch: 13/15 Iteration: 555 Train loss: 0.002\n",
      "Epoch: 13/15 Iteration: 560 Train loss: 0.001\n",
      "Epoch: 14/15 Iteration: 565 Train loss: 0.339\n",
      "Epoch: 14/15 Iteration: 570 Train loss: 0.371\n",
      "Epoch: 14/15 Iteration: 575 Train loss: 0.205\n",
      "Val acc: 0.695\n",
      "Epoch: 14/15 Iteration: 580 Train loss: 0.092\n",
      "Epoch: 14/15 Iteration: 585 Train loss: 0.049\n",
      "Epoch: 14/15 Iteration: 590 Train loss: 0.059\n",
      "Epoch: 14/15 Iteration: 595 Train loss: 0.029\n",
      "Epoch: 14/15 Iteration: 600 Train loss: 0.033\n",
      "Val acc: 0.846\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
